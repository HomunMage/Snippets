---
layout: board
title:  "資料用盡"
date:   2023-04-26 10:00:00 +0800
categories: [Theory]
---

<iframe width="450" height="255" src="https://www.youtube.com/embed/c4aR_smQgxY" title="YouTube video player" frameborder="0" ></iframe>

AI可以訓練需要三個要素 算力 算法 高品質訓練資料。  
近年GPU才有足夠的算力。  
訓練資料一直以來都是難以準備的，所以GPT3剛出來的時候，可以看到研究所對於GPT3的評價都是演算法不是什麼新鮮的架構。  

GPT3,4 會只使用到2021也是為了避免汙染，因為2022之後的AI生產內容變多，  
GPT5因為使用了影片當作訓練資料，同樣也是為了避免2022之後大量出現的AI生產，也是採用2021以前的影片。

本來人類知識和媒體在近百年內爆炸成長，GPT5居然變成了不夠用了XD

雖然AI在訓練中有一些資料放大技術 Data Augmentation  然後也會有模型壓縮技術Quantization  
但是在GPT5或許只是杯水車薪

這個現象其實非常有趣，當初大數據(big data)會炒起來，是因為資料太多，無法使用傳統的統計學方法，所以發展出資料探勘(data mining)技術，結合機器學習(machine learning)技術，發展到了現在深度學習(deep learning)的可無限scalability的架構，居然變成了資料不夠XD

我猜未來可能有幾種可能方向，  
第一個方向  
照目前的 GPT 1-> 2 -> 3 -> 4 -> 5 -> 6都是模型越來越大，同一個架構下勢必要進入使用AI生產優秀資料的時代，為了訓練GPT6需要遠比目前人類數據總和的優秀內容。  
比如由AI拿著攝影機走在路上去拍360影片、讓AI去當服務生跟全世界對話蒐集資料、讓AI去做大量實驗去蒐集大量資料，以這些量產資料拿去餵GPT6

第二個方向  
因為GPT5是AGI，讓GPT5去設計一種全新的AI訓練/學習架構，同時硬體也由AGI做一種全新的設計，  
形成的新架構就不需要遇到資料耗盡的問題  
同時因為GPT5參與科學研究，加速了其他類型硬體的進展，比如量子電腦和核融合技術  
下一代的AGI則可能使用截然不同的AI架構。   
